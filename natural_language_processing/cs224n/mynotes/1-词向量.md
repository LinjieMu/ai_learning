## 词向量

据估计，英语有1300万个token，但它们都是完全无关的吗？Feline到cat，hotel到motel？当时不是。因此，我们希望将每个单词token编码为在某种有序“单词”空间内的一个点向量。这是至关重要的，原因有很多，但最直观的原因是，可能实际上存在一些N维空间（N<<1300万），足以编码我们的语言的所有语义。每个维度都会编码一些我们使用语音传递的含义。

因此，让我们深入研究我们的第一个单词向量（Word Vectors），可以说是最简单的，**one-hot向量**：将每个单词表示为一个$\mathbb{R}^{\left|V\right|\times1}$向量，该向量只有一个元素为1，其余全为0。在这种表示法中，$\left|V\right|$是词汇表的大小。这种编码形式的词向量如下所示:
$$
w^{\text {aardvark }}=\left[\begin{array}{c}
1 \\
0 \\
0 \\
\vdots \\
0
\end{array}\right], w^a=\left[\begin{array}{c}
0 \\
1 \\
0 \\
\vdots \\
0
\end{array}\right], w^{a t}=\left[\begin{array}{c}
0 \\
0 \\
1 \\
\vdots \\
0
\end{array}\right], \cdots w^{z e b r a}=\left[\begin{array}{c}
0 \\
0 \\
0 \\
\vdots \\
1
\end{array}\right]
$$
我们将每个单词表示为一个完全独立的实体。如前所述，这种单词表示方式没有直接给出任何相似度的概念。因此，也许我们可以尝试将这个空间的大小从$\mathbb{R}^{\left|V\right|}$减到更小，从而找到一个子空间来编码单词之间的关系。

## 基于SVD的方法

对于这类寻找词嵌入的方法，我们首先循环一个大规模数据集，并在某种形式的矩阵$X$中累加单词共现的次数，然后对$X$执行奇异值分解以获得$USV^T$。然后，我们使用$U$的行作为字典中所有单词的单词嵌入。让我们讨论$X$的几种选择。

### 1. Word-Document矩阵

作为我们的第一次尝试，我们做了一个大胆的猜想：<u>同一份文档中的单词都是相关的</u>。例如,“银行”、“债券”、“股票”、“货币”等很可能同时出现。但是“银行”、“章鱼”、“香蕉”和“曲棍球”可能不会一直出现在一起。我们利用这个事实来构建一个**Word-Document矩阵**$X$。按以下方式循环：在文档$j$中，单词$i$每出现一次，我们就给条目$X_{ij}$加$1$。这显然是一个很大的矩阵（$\mathbb{R}^{|V|\times M}$），它与文档数量（$M$）成比例。所以也许我们可以尝试更好的方法。

### 2. 基于窗口的共现矩阵

同样的逻辑也适用于该方法，矩阵$X$的存储单词的共现数。本方法中，我们计算每个单词在感兴趣单词周围一个特定大小的窗口中出现的次数。我们计算语料库中所有单词的共现次数。我们在下面展示一个例子，语料库只包含三个句子，窗口大小为1:

>I enjoy flying.  
>
>I like NLP.  
>
>I like deep learning.  

共现矩阵（Co-occurrence Matrix）如下：
$$
X = \left[\begin{array}{llllllll}
0 & 2 & 1 & 0 & 0 & 0 & 0 & 0 \\
2 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 & 1 & 1 & 0
\end{array}\right]
$$

### 3. 将SVD应用于共现矩阵

现在我们对$X$执行$SVD$，观察奇异值(结果$S$矩阵中的对角线元素)，并根据期望的方差百分比在索引$k$处截断它们:
$$
\frac{\sum_{i=1}^k \sigma_i}{\sum_{i=1}^{|V|} \sigma_i}
$$
然后我们取$U_{1:|V|, 1: k}$的子矩阵作为我们的词嵌入矩阵。因此，这将为我们提供词汇表中每个单词的$k$维表示。

**对矩阵$X$应用$SVD$**：

![image-20230128154758016](https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230128154758016.png)

**通过选择前k个奇异向量来降低维度:**

![image-20230128155003731](https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230128155003731.png)

这两种方法给出的词向量足以编码语义和句法(词性)信息，但也会带来许多其他问题。

- 矩阵的维度变化非常频繁(新词添加非常频繁，语料库的大小也会变化)。
- 矩阵非常稀疏，因为大多数单词都没有共现。
- 矩阵通常是非常高维的($\approx 10^6 \times 10^6$)
- 二次训练的成本(即执行$SVD$)
- 需要在$X$上进行一些调整来解释词频的严重不平衡

解决上述问题的一些解决方案：

- 忽略一些功能词，如"the", "he", "has"等
- 应用斜坡窗口（ramp window），例如，基于文档中单词之间的距离对共现次数进行加权
- 使用皮尔逊相关并将负计数设置为0，而不是仅使用原始计数。

## 基于迭代的方法 - Word2vec

让我们退后一步，尝试一种新的方法。与其计算和存储关于某个大型数据集(可能是数十亿个句子)的全局信息，我们可以尝试创建一个模型，该模型将能够一次学习一个迭代，并最终能够对给定上下文的单词的概率进行编码。

这个想法是设计一个模型，其参数是词向量。然后，在某个训练集上训练该模型。在每次迭代中，评估模型的误差，并遵循一个更新规则调整参数。我们将这种方法称为误差的“反向传播”。模型和任务越简单，训练它的速度就越快。

下面，我们将介绍一种更简单、更近期的概率方法: $word2vec$。$Word2vec$是一系列方法的集合，实际上包括: 

- **算法**：连续词袋模型（$CBOW$）和$skip-gram$模型。$CBOW$旨在根据上下文的词向量预测出中心词。$Skip-gram$则相反，它从中心单词预测上下文词的分布(概率)。
- **训练方法**：负采样和分层$softmax$。负采样通过对负示例进行采样来定义目标，而分层$softmax$使用高效的树结构来计算所有词汇表的概率来定义目标。

$Word2vec$是一个学习词向量的框架，该框架的设计思路为:

- 我们有一个很大的文本语料库（corpus）
- 词汇表中的每个单词都用固定的向量（vector）表示
- 对于文本中的每个位置t，它都有一个中心单词c和众多上下文单词o
- 使用c和o的词向量的相似度，来计算给定c情况下上下文o出现的概率
-  不断调整词向量以最大化此概率

### 1. 语言模型

首先，我们需要创建这样一个模型，它将为单词序列分配概率。让我们从一个例子开始:

> "The cat jumped over the puddle."  

一个好的语言模型会给这个句子一个很高的概率，因为这是一个完全有效的句子，无论从语法上还是语义上。类似地，“stock boil fish is toy”这句话的概率应该很低，因为它没有意义。从数学上讲，我们可以把任意n个单词组成的序列的概率称为:
$$
P\left(w_1, w_2, \cdots, w_n\right)
$$
我们可以采用一元语言模型的方法，通过假设单词出现的次数是完全独立的来分解这个概率：
$$
P\left(w_1, w_2, \cdots, w_n\right)=\prod_{i=1}^n P\left(w_i\right)
$$
然而，我们知道这有点可笑，因为下一个单词在很大程度上取决于前一个单词序列。而那些愚蠢的例子可能会得到很高的分数。也许我们让序列的概率取决于序列中一个单词和它旁边的单词的成对概率更为合适。我们称之为双构成词模型（bigram model），并表示为：
$$
P\left(w_1, w_2, \cdots, w_n\right)=\prod_{i=2}^n P\left(w_i \mid w_{i-1}\right)
$$
同样，这依然有点幼稚，因为我们只关心相邻单词对，而不是评估整个句子。但正如我们将看到的，这种表示变得更加漂亮。注意，在上下文大小为1的词-词矩阵中，我们基本上可以学习这些成对的概率。但是，这会需要在一个巨大的数据集上计算和存储海量的全局信息。

现在我们理解了如何考虑具有概率的标记序列，让我们观察一些可以学习这些概率的示例模型。

### 2. 连续词袋模型

一种方法是将{"The"， "cat"， " over"， "The"， "puddle"}作为上下文，从这些单词中，能够预测或生成中心单词"jumped"。我们把这种模型称为连续词袋模型(Continuous Bag of Words, CBOW)。

下面，详细地讨论一下CBOW模型。首先，我们设置已知的参数。让已知参数变成由one-hot词向量表示的句子。用$x^{(c)}$表示输入的one-hot向量或上下文，$y^{(c)}$表示输出。因为在CBOW模型中只有一个输出$y$，所以我们称它为中心词的一个one-hot向量。现在让我们定义模型中的未知数。

我们创建两个矩阵，$\mathcal{V} \in \mathbb{R}^{n \times|V|}$和 $\mathcal{U} \in \mathbb{R}^{|V| \times n}$。其中$n$是一个表示任意大小的整数，它定义了嵌入空间的大小。$\mathcal{V}$是输入的词矩阵。当$\mathcal{V}$是这个模型的输入时，$\mathcal{V}$的第$i$列是单词$w_i$的$n$维嵌入向量。我们将这个$n\times 1$向量表示为$v_i$。类似地，$U$是输出的词矩阵。当$U$是模型的输出时，它的第$j$行是单词$w_j$的$n$维嵌入向量。我们把这一行$U$表示为$u_j$。请注意，实际上我们为每个单词$w_i$学习了两个向量(即输入单词向量$v_i$和输出单词向量$u_i$)。

我们将该模型的工作方式分解为以下步骤:

>1. 给大小为$m$的输入上下文生成one-hot向量$\left(x^{(c-m)}, \ldots, x^{(c-1)}, x^{(c+1)}, \ldots, x^{(c+m)} \in \mathbb{R}^{|V|}\right)$
>2. 获取上下文的嵌入词向量$\left(v_{C-m}=\right.\left.\mathcal{V} x^{(c-m)}, v_{c-m+1}=\mathcal{V} x^{(c-m+1)}, \ldots, v_{c+m}=\mathcal{V} x^{(c+m)} \in \mathbb{R}^{n}\right)$
>3. 对这些向量进行平均，得到$\hat{v}=\frac{v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}}{2 m} \in \mathbb{R}^{n}$
>4. 生成得分向量$z=\mathcal{U} \hat{v} \in \mathbb{R}^{|V|}$。由于相似向量的点积较高，它会将相似的单词推近，以获得较高的分数
>5. 将分数转换为概率$\hat{y}=\operatorname{softmax}(z) \in \mathbb{R}^{|V|}$
>6. 我们希望生成的概率$\hat{y} \in \mathbb{R}^{|V|}$与真实概率$y \in \mathbb{R}^{|V|}$相匹配，这也恰好是实际单词的one-hot向量。

现在已经了解了在矩阵$\mathcal{V}$和$\mathcal{U}$已知的情况下，CBOW模型是如何工作的。那么，如何学习这两个矩阵呢？我们需要创建一个目标函数，使用距离/损失测度：交叉熵$H(\hat{y}, y)$。

对于在离散情况下使用交叉熵的公式可以从损失函数的公式中推导出来:
$$
H(\hat{y}, y)=-\sum_{j=1}^{|V|} y_j \log \left(\hat{y}_j\right)
$$
由于$y$是一个one-hot向量。由此可知，上述损失简化为:
$$
H(\hat{y}, y)=-y_c \log \left(\hat{y}_c\right)
$$
在这个公式中，$c$是正确单词的one-hot向量为1的索引（$y_c=1$）。可以看到，对于概率分布，交叉熵为我们提供了一个很好的距离测量方法。对于每一个单词$w$，我们使用两个向量进行表示：中心词$v_w$和上下文$u_w$。对于一个中心词$c$和上下文$o$，从softmax中获取灵感，其概率计算公式如下：
$$
P(o \mid c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}
$$
由上面两个结论可以推出目标函数：
$$
\begin{aligned}
\operatorname*{minimize}J &= -\log P(w_{c}|w_{c-m},\cdot\cdot\cdot,w_{c-1},w_{c+1},\cdot\cdot\cdot,w_{c+m})\\ 
&=-\log P(u_{c}|\hat{v}) \\ 
&=-\log\frac{\exp(u_{c}^{T}\hat{v})}{{\sum_{j=1}^{|V|}\exp(u_{j}^{T}\hat{v})}}\\ 
&=\begin{aligned}-u_c^T\hat{v}+\log\sum_{j=1}^{|V|}\exp(u_j^T\hat{v})\end{aligned}
\end{aligned}
$$
使用随机梯度下降来更新所有相关的词向量$u_c$和$v_j$。

### 3. Skip-Gram模型

另一种方法是创建一个模型，使得给定中心单词"jumped"，该模型将能够预测或生成周围的单词"the"，"cat"，"over"，"the"，"puddle"。我们称这些单词为"jumped"上下文。我们称这种类型的模型为**Skip-Gram模型**。

让我们讨论一下上面的Skip-Gram模型。设置在很大程度上是相同的，但我们本质上交换了$x$和$y$。输入的一个one-hot向量(中心单词)用$x$表示(因为只有一个)。输出向量用$y^{(j)}$表示。$\mathcal{V}$和$\mathcal{U}$的定义和CBOW一样。

我们将这个模型的工作方式分解为以下6个步骤：

>1. 生成中心单词的one-hot向量$x\in\mathbb{R}^{|V|}$
>2. 得到中心词的词嵌入向量$v_{c}=\mathcal{V}x\in \mathbb{R}^n$
>3. 生成得分向量$z=\mathcal{U}v_c$
>4. 将分数向量转换为概率，$\hat y=\text{softmax}(z)$。$\hat{y}_{c-m},\ldots,\hat{y}_{c-1},\hat{y}_{c+1},\ldots,\hat{y}_{c+m}$是观察到每个上下文词的概率
>5. 我们希望生成的概率向量与真实概率匹配，$y^{(c-m)},...,y^{(c-1)},y^{(c+1)},...,y^{(c+m)}$是实际输出的one-hot向量。

与CBOW一样，我们需要生成一个目标函数来评估模型。这里的一个关键区别是我们调用朴素贝叶斯假设来划分概率。如果你以前没有见过，简单地说，这是一个强(朴素)条件独立性假设。换句话说，给定中心词，所有输出单词是完全独立的。
$$
\begin{aligned}
\operatorname{minimize} J & =-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} \mid w_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} \mid w_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} \mid v_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} \frac{\exp \left(u_{c-m+j}^T v_c\right)}{\sum_{k=1}^{|V|} \exp \left(u_k^T v_c\right)} \\
& =-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^T v_c+2 m \log \sum_{k=1}^{|V|} \exp \left(u_k^T v_c\right)
\end{aligned}
$$
有了这个目标函数，我们可以计算关于未知参数的梯度，并在每次迭代中通过随机梯度下降更新它们。

注意，
$$
\begin{aligned}
J & =-\sum_{j=0, j \neq m}^{2 m} \log P\left(u_{c-m+j} \mid v_c\right) \\
& =\sum_{j=0, j \neq m}^{2 m} H\left(\hat{y}, y_{c-m+j}\right)
\end{aligned}
$$
其中，$H\left(\hat{y}, y_{c-m+j}\right)$是概率向量$\hat{y}$和one-hot向量$y_{c-m+j}$上的交叉熵。

### 4. 负采样

让我们来看看目标函数。请注意$\left|V\right|$上的求和计算量非常大！我们做的任何更新或目标函数的评估都需要$O(\left|V\right|)$的时间。回忆一下，$\left|V\right|$的规模在百万级别。一个简单的想法是我们可以进行近似计算。

对于每个训练步骤，与其遍历整个词汇表，倒不如只采样几个负例。我们从噪声分布$P_n(w)$（其概率与词汇表频率的排序相匹配）中“采样”。为了扩充我们引入负采样的问题的公式，需要调整目标函数、梯度、更新策略。

虽然负采样是基于Skip-Gram模型的，但它实际上是在优化一个不同的目标。考虑一个单词-上下文对$(w,c)$。这一对来自训练数据吗？我们用$P(D=1|w, c)$表示$(w,c)$来自语料库数据的概率。相应地，我们用$P(D=0|w, c)$表示$(w,c)$不来自语料库数据的概率。首先，我们使用sigmoid函数对$P(D=1|w, c)$进行建模：
$$
P(D=1|w,c,\theta)=\sigma(v_c^T v_w)=\dfrac{1}{1+e^{(-v_c^T v_w)}}
$$
现在，我们构建一个新的目标函数，如果一个单词和上下文确实存在于语料库数据中，则尝试最大化它存在的概率。如果它确实不在语料库数据中，则最大化它不存在的概率。我们对这两个概率采用一个简单的最大似然方法。(这里我们取$\theta$作为模型的参数，在本例中是$\mathcal{V}$和$\mathcal{U}$。)
$$
\begin{aligned} \theta & =\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}} P(D=0 \mid w, c, \theta) \\ & =\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}}(1-P(D=1 \mid w, c, \theta)) \\ & =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 \mid w, c, \theta)+\sum_{(w, c) \in \tilde{D}} \log (1-P(D=1 \mid w, c, \theta)) \\ & =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_w^T v_c\right)}\right) \\ & =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_w^T v_c\right)}\right) \end{aligned}
$$
注意，最大化似然与最小化负对数似然相同：
$$
J=-\sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}-\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_w^T v_c\right)}\right)
$$
请注意，$\tilde{D}$是一个“假的”或“否定的”语料库。像“高汤煮鱼是玩具”这样的不自然的句子，出现的概率很低。我们可以通过从词库中随机抽样来生成$\tilde{D}$。

对于skip-gram来说，在给定用来观察上下文词$c−m + j$的中心单词$c$的情况下，新目标函数是：
$$
-\log \sigma\left(u_{c-m+j}^T \cdot v_c\right)-\sum_{k=1}^K \log \sigma\left(-\tilde{u}_k^T \cdot v_c\right)
$$
对于CBOW来说，在给定用来观察中心词$u_c$的上下文向量$\hat{v}=\frac{v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}}{2m}$的情况下的，新的目标函数为：
$$
\begin{aligned}-\log\sigma(u_c^T\cdot\hat{v})-\sum\limits_{k=1}^K\log\sigma(-\tilde{u}_k^T\cdot\hat{v})\end{aligned}
$$
在上述公式中，$\{\tilde{u}_k|\text{}k=1\ldots K\}$取自$P_n(w)$。

### 5. 分层Softmax

在实践中，分层softmax往往对低频词更有效，而负采样对高频词和低维向量更有效。

分层softmax使用二叉树来表示词汇表中的所有单词。树的每个叶子都是一个单词，从根到叶子有一条唯一的路径。在这个模型中，没有单词的输出表示。相反，图的每个节点(除了根节点和叶节点)都与模型将要学习的向量相关联。

在这个模型中，单词$w$的向量为$w_i$的概率$P(w|w_i)$等于随机从根节点开始，结束于$w_i$对应的叶节点的概率。用这种方法计算概率的主要优点是代价仅为$O(\log(|V|))$，对应于路径的长度。

让我们引入一些符号。设$L(w)$为从根节点到叶子节点$w$的路径的节点数（不算根节点）。设这条路径上的第$i$个节点$n(w,i)$的向量为$v_{n(w,i)}$。所以$n(w, 1)$是根节点，而$n(w,L(w))$是$w$的父节点。现在对于每个内部节点$n$，我们任意选择它的一个子节点并称之为$\text{ch}(n)$。然后，我们可以计算概率为：
$$
P\left(w \mid w_i\right)=\prod_{j=1}^{L(w)-1} \sigma\left([n(w, j+1)=\operatorname{ch}(n(w, j))] \cdot v_{n(w, j)}^T v_{w_i}\right)
$$
其中，
$$
[x]=\begin{cases}1\text{ if }x\text{ is true}\\ -1\text{ otherwise}\end{cases}
$$
$\sigma(\cdot)$是sigmoid函数。

这个公式非常稠密，让我们更仔细地研究一下

首先，我们根据根节点$(n(w, 1))$到叶节点$\text{leaf}(w)$路径的形状计算乘积。如果我们假设$\text{ch}(n)$总是$n$的左节点，那么当路径向左时，术语$[n(w, j + 1) = \text{ch}(n(w, j))]$返回1，如果向右，返回-1。

此外，术语$[n(w, j + 1) = \text{ch}(n(w, j))]$提供了规范化。在节点n处，如果我们将前往左节点和右节点的概率相加，对于任意值的$v_n^Tv_{wi}$，有
$$
\sigma(v_n^T v_{w_i})+\sigma(-v_n^T v_{w_i})=1
$$
归一化还确保$\sum_{w=1}^{|V|}P(w|w_i)=1\text{}$，就像原始的softmax一样。

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230128194415794.png" alt="image-20230128194415794" style="zoom:50%;" />

最后，我们使用点积比较输入向量$v_{wi}$与每个内部节点向量$v^T_{n(w,j)}$的相似性。让我们来看一个例子。以图中的$w_2$为例，我们必须先取两条左边，再取一条右边，才能从根结点到达$w_2$，因此：
$$
\begin{aligned}P(w_2|w_i)&=p(n(w_2,1),\text{left})\cdot p(n(w_2,2),\text{left})\cdot p(n(w_2,3),\text{right})\\ &=\sigma(v_{n(w_2,1)}^Tv_{w_i})\cdot\sigma(v_{n(w_2,2)}^T v_{w_i})\cdot\sigma(-v_{n(w_2,3)}^T v_{w_i})\end{aligned}
$$
为了训练模型，我们的目标仍然是最小化负对数似然$1-\log P(w|w_i)$。但我们不是更新每个单词的输出向量，而是更新二叉树中从根节点到叶节点路径上的节点的向量。

该方法的速度取决于二叉树的构建方式和叶节点的单词分配方式。

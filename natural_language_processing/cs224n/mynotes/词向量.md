# 词向量的嵌入

据估计，英语有1300万个单词，它们都是语义无关的吗？Feline和cat，hotel和motel？当时不是。因此，我们希望将每个单词token编码为在某种有序“单词”空间内的一个点向量。这是至关重要的，但最直观的原因是，可能存在一些N维空间（N<<1300万），足以编码英语中所有单词的语义。每个维度都会编码单词的一些含义。

因此，让我们深入研究第一个词向量（Word Vectors），可以说是最简单的，**one-hot向量**：将每个单词表示为一个$\mathbb{R}^{\left|V\right|\times1}$向量，该向量只有一个元素为1，其余全为0。在这种表示法中，$\left|V\right|$是词汇表的大小。这种编码形式的词向量如下所示:
$$
w^{\text {aardvark }}=\left[\begin{array}{c}
1 \\
0 \\
0 \\
\vdots \\
0
\end{array}\right], w^a=\left[\begin{array}{c}
0 \\
1 \\
0 \\
\vdots \\
0
\end{array}\right], w^{a t}=\left[\begin{array}{c}
0 \\
0 \\
1 \\
\vdots \\
0
\end{array}\right], \cdots w^{z e b r a}=\left[\begin{array}{c}
0 \\
0 \\
0 \\
\vdots \\
1
\end{array}\right]
$$
该编码将每个单词表示为一个完全独立的实体。如前所述，这种单词表示方式没有直接给出任何相似度的概念。因此，也许可以尝试将这个空间的大小从$\mathbb{R}^{\left|V\right|}$减到更小，从而找到一个子空间来编码单词之间的关系。

## 基于SVD的方法

对于这类寻找词嵌入的方法，我们首先循环一个大规模数据集，并在某种形式的矩阵$X$中累加单词共现的次数，然后对$X$执行奇异值分解以获得$\text{USV}^\text{T}$。然后，我们使用$U$的行作为字典中所有单词的单词嵌入。让我们讨论$X$的几种选择。

### 1. Word-Document矩阵

一个大胆的猜想：<u>同一份文档中的单词都是相关的</u>。例如,“银行”、“债券”、“股票”、“货币”等很可能同时出现。但是“银行”、“章鱼”、“香蕉”和“曲棍球”可能不会一直出现在一起。利用这个事实来构建一个**Word-Document矩阵**$X$。按以下方式循环：在文档$j$中，单词$i$每出现一次，我们就给条目$X_{ij}$加$1$。这显然是一个很大的矩阵（$\mathbb{R}^{|V|\times M}$），它与文档数量（$M$）成比例。所以也许可以尝试更好的方法。

### 2. 基于窗口的共现矩阵

同样的逻辑也适用于该方法，矩阵$X$的存储单词的共现数。在该方法中，我们计算每个单词在感兴趣单词周围一个特定大小的窗口中出现的次数。计算语料库中所有单词的共现次数。在下面展示一个例子，语料库只包含三个句子，窗口大小为1:

>I enjoy flying.  
>
>I like NLP.  
>
>I like deep learning.  

**共现矩阵**（Co-occurrence Matrix）如下：
$$
X = \left[\begin{array}{llllllll}
0 & 2 & 1 & 0 & 0 & 0 & 0 & 0 \\
2 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 & 1 & 1 & 0
\end{array}\right]
$$

### 3. 将SVD应用于共现矩阵

现在对$X$执行$\text{SVD}$，观察奇异值(结果$S$矩阵中的对角线元素)，并根据期望的方差百分比在索引$k$处截断它们:
$$
\frac{\sum_{i=1}^k \sigma_i}{\sum_{i=1}^{|V|} \sigma_i}
$$
然后我们取$U_{1:|V|, 1: k}$的子矩阵作为我们的词嵌入矩阵。因此，这将提供词汇表中每个单词的$k$维表示。

**对矩阵$X$应用$\text{SVD}$**：

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230128154758016.png" alt="image-20230128154758016" style="zoom:33%;" />

**通过选择前k个奇异向量来降低维度:**

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230128155003731.png" alt="image-20230128155003731" style="zoom:33%;" />

这两种方法给出的词向量足以编码语义和句法(词性)信息，但也会带来许多其他问题。

- 矩阵的维度变化非常频繁(新词添加非常频繁，语料库的大小也会变化)。
- 矩阵非常稀疏，因为大多数单词都没有共现。
- 矩阵通常是非常高维的($\approx 10^6 \times 10^6$)
- 二次训练的成本(即执行$\text{SVD}$)
- 需要在$X$上进行一些调整来解释词频的严重不平衡

解决上述问题的一些解决方案：

- 忽略一些功能词，如"the", "he", "has"等
- 应用斜坡窗口（ramp window），例如，基于文档中单词之间的距离对共现次数进行加权
- 使用皮尔逊相关并将负计数设置为0，而不是仅使用原始计数。

## 基于迭代的方法 - Word2vec

与其计算和存储关于某个大型数据集(可能是数十亿个句子)的全局信息，我们可以尝试创建一个模型，该模型能够一次学习一个迭代，最终能够对给定上下文的单词的概率进行编码。

利用该想法设计一个模型，其参数是词向量。然后，在某个训练集上训练该模型。在每次迭代中，评估模型的误差，并遵循一个更新规则调整参数。我们将这种方法称为误差的“**反向传播**”。模型和任务越简单，训练它的速度就越快。

下面，我们将介绍一种更简单、更先进的概率方法: $\text{word2vec}$。$\text{word2vec}$是一系列方法的集合，实际上包括: 

- **算法**：连续词袋模型（$\text{CBOW}$）和$\text{skip-gram}$模型。$\text{CBOW}$旨在根据上下文的词向量预测出中心词。$\text{skip-gram}$则相反，它从中心单词预测上下文词的分布(概率)。
- **训练方法**：负采样和分层$\text{softmax}$。负采样通过对负示例进行采样来定义目标函数，而分层$\text{softmax}$使用高效的树结构来计算所有词汇表的概率来定义目标函数。

$\text{word2vec}$是一个学习词向量的框架，该框架的设计思路为:

- 我们有一个很大的文本语料库（corpus）
- 词汇表中的每个单词都用固定的向量（vector）表示
- 对于文本中的每个位置t，它都有一个中心单词c和众多上下文单词o
- 使用c和o的词向量的相似度，来计算给定c情况下上下文o出现的概率
-  不断调整词向量以最大化此概率

### 1. 语言模型

首先，创建这样一个模型，它将为单词序列分配概率。让我们从一个例子开始:

> "The cat jumped over the puddle."  

一个好的语言模型会给这个句子一个很高的概率，因为这是一个完全有效的句子，无论从语法上还是语义上。类似地，“stock boil fish is toy”这句话的概率应该很低，因为它没有实际意义。从数学上讲，可以把任意n个单词组成的序列的概率称为:
$$
P\left(w_1, w_2, \cdots, w_n\right)
$$
我们可以采用一元语言模型的方法，通过假设单词出现的次数是完全独立的来分解这个概率：
$$
P\left(w_1, w_2, \cdots, w_n\right)=\prod_{i=1}^n P\left(w_i\right)
$$
然而，这有点可笑，因为下一个单词在很大程度上取决于前一个单词序列。也许让序列的概率取决于序列中一个单词和它旁边的单词的成对概率更为合适。我们称之为**双构成词模型**（bigram model），并表示为：
$$
P\left(w_1, w_2, \cdots, w_n\right)=\prod_{i=2}^n P\left(w_i \mid w_{i-1}\right)
$$
同样，这依然有点幼稚，因为我们只关心相邻单词对，而不是评估整个句子。但正如我们将看到的，这种表示变得更加漂亮。注意，在上下文大小为1的词-词矩阵中，我们基本上可以学习这些成对的概率。但是，这会需要在一个巨大的数据集上计算和存储海量的全局信息。

### 2. 连续词袋模型

一种方法是将{"The"， "cat"， " over"， "The"， "puddle"}作为上下文，从这些单词中，能够预测或生成中心单词"jumped"。我们把这种模型称为**连续词袋模型**(Continuous Bag of Words, $ \text{CBOW}$)。

下面，详细地讨论一下$\text{CBOW}$模型。首先，我们设置已知的参数。让已知参数变成由one-hot词向量表示的句子。用$x^{(c)}$表示输入的one-hot向量或上下文，$y^{(c)}$表示输出。因为在$\text{CBOW}$模型中只有一个输出$y$，所以我们称它为中心词的一个one-hot向量。现在让我们定义模型中的未知数。

我们创建两个矩阵，$\mathcal{V} \in \mathbb{R}^{n \times|V|}$和 $\mathcal{U} \in \mathbb{R}^{|V| \times n}$。其中$n$是一个表示任意大小的整数，它定义了嵌入空间的大小。$\mathcal{V}$是输入的词矩阵。当$\mathcal{V}$是这个模型的输入时，$\mathcal{V}$的第$i$列是单词$w_i$的$n$维嵌入向量。我们将这个$n\times 1$向量表示为$v_i$。类似地，$U$是输出的词矩阵。当$U$是模型的输出时，它的第$j$行是单词$w_j$的$n$维嵌入向量。我们把这一行$U$表示为$u_j$。请注意，实际上我们为每个单词$w_i$学习了两个向量(即输入单词向量$v_i$和输出单词向量$u_i$)。

我们将该模型的工作方式分解为以下步骤:

>1. 给大小为$m$的输入上下文生成one-hot向量$\left(x^{(c-m)}, \ldots, x^{(c-1)}, x^{(c+1)}, \ldots, x^{(c+m)} \in \mathbb{R}^{|V|}\right)$
>2. 获取上下文的嵌入词向量$\left(v_{C-m}=\right.\left.\mathcal{V} x^{(c-m)}, v_{c-m+1}=\mathcal{V} x^{(c-m+1)}, \ldots, v_{c+m}=\mathcal{V} x^{(c+m)} \in \mathbb{R}^{n}\right)$
>3. 对这些向量进行平均，得到$\hat{v}=\frac{v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}}{2 m} \in \mathbb{R}^{n}$
>4. 生成得分向量$z=\mathcal{U} \hat{v} \in \mathbb{R}^{|V|}$。由于相似向量的点积较高，它会将相似的单词推近，以获得较高的分数
>5. 将分数转换为概率$\hat{y}=\operatorname{softmax}(z) \in \mathbb{R}^{|V|}$
>6. 我们希望生成的概率$\hat{y} \in \mathbb{R}^{|V|}$与真实概率$y \in \mathbb{R}^{|V|}$相匹配，这也恰好是实际单词的one-hot向量。

现在已经了解了在矩阵$\mathcal{V}$和$\mathcal{U}$已知的情况下，$\text{CBOW}$模型是如何工作的。那么，如何学习这两个矩阵呢？我们需要创建一个目标函数，使用距离/损失测度：交叉熵$H(\hat{y}, y)$。

对于在离散情况下使用交叉熵的公式可以从损失函数的公式中推导出来:
$$
H(\hat{y}, y)=-\sum_{j=1}^{|V|} y_j \log \left(\hat{y}_j\right)
$$
由于$y$是一个one-hot向量。由此可知，上述损失简化为:
$$
H(\hat{y}, y)=-y_c \log \left(\hat{y}_c\right)
$$
在这个公式中，$c$是正确单词的one-hot向量为1的索引（$y_c=1$）。可以看到，对于概率分布，交叉熵为我们提供了一个很好的距离测量方法。对于每一个单词$w$，我们使用两个向量进行表示：中心词$v_w$和上下文$u_w$。对于一个中心词$c$和上下文$o$，从$\text{softmax}$中获取灵感，其概率计算公式如下：
$$
P(o \mid c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}
$$
由上面两个结论可以推出目标函数：
$$
\begin{aligned}
\operatorname*{minimize}J &= -\log P(w_{c}|w_{c-m},\cdot\cdot\cdot,w_{c-1},w_{c+1},\cdot\cdot\cdot,w_{c+m})\\ 
&=-\log P(u_{c}|\hat{v}) \\ 
&=-\log\frac{\exp(u_{c}^{T}\hat{v})}{{\sum_{j=1}^{|V|}\exp(u_{j}^{T}\hat{v})}}\\ 
&=\begin{aligned}-u_c^T\hat{v}+\log\sum_{j=1}^{|V|}\exp(u_j^T\hat{v})\end{aligned}
\end{aligned}
$$
使用随机梯度下降来更新所有相关的词向量$u_c$和$v_j$。

### 3. Skip-Gram模型

另一种方法是创建一个模型，使得给定中心单词"jumped"，该模型将能够预测或生成周围的单词"the", "cat", "over", "the", "puddle"。我们称这些单词为"jumped"上下文。我们称这种类型的模型为**Skip-Gram模型**。

让我们讨论一下上面的$\text{skip-gram}$模型。设置在很大程度上是相同的，但我们本质上交换了$x$和$y$。输入的一个one-hot向量(中心单词)用$x$表示(因为只有一个)。输出向量用$y^{(j)}$表示。$\mathcal{V}$和$\mathcal{U}$的定义和$\text{CBOW}$一样。

我们将这个模型的工作方式分解为以下6个步骤：

>1. 生成中心单词的one-hot向量$x\in\mathbb{R}^{|V|}$
>2. 得到中心词的词嵌入向量$v_{c}=\mathcal{V}x\in \mathbb{R}^n$
>3. 生成得分向量$z=\mathcal{U}v_c$
>4. 将分数向量转换为概率，$\hat y=\text{softmax}(z)$。$\hat{y}_{c-m},\ldots,\hat{y}_{c-1},\hat{y}_{c+1},\ldots,\hat{y}_{c+m}$是观察到每个上下文词的概率
>5. 我们希望生成的概率向量与真实概率匹配，$y^{(c-m)},...,y^{(c-1)},y^{(c+1)},...,y^{(c+m)}$是实际输出的one-hot向量。

与$\text{CBOW}$一样，我们需要生成一个目标函数来评估模型。这里的一个关键区别是我们调用朴素贝叶斯假设来划分概率。如果你以前没有见过，简单地说，这是一个强(朴素)条件独立性假设。换句话说，给定中心词，所有输出单词是完全独立的。
$$
\begin{aligned}
\operatorname{minimize} J & =-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} \mid w_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} \mid w_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} \mid v_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} \frac{\exp \left(u_{c-m+j}^T v_c\right)}{\sum_{k=1}^{|V|} \exp \left(u_k^T v_c\right)} \\
& =-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^T v_c+2 m \log \sum_{k=1}^{|V|} \exp \left(u_k^T v_c\right)
\end{aligned}
$$
有了这个目标函数，我们可以计算关于未知参数的梯度，并在每次迭代中通过随机梯度下降更新它们。

注意，
$$
\begin{aligned}
J & =-\sum_{j=0, j \neq m}^{2 m} \log P\left(u_{c-m+j} \mid v_c\right) \\
& =\sum_{j=0, j \neq m}^{2 m} H\left(\hat{y}, y_{c-m+j}\right)
\end{aligned}
$$
其中，$H\left(\hat{y}, y_{c-m+j}\right)$是概率向量$\hat{y}$和one-hot向量$y_{c-m+j}$上的交叉熵。

### 4. 负采样

让我们来看看目标函数。请注意$\left|V\right|$上的求和计算量非常大！我们做的任何更新或目标函数的评估都需要$O(\left|V\right|)$的时间。回忆一下，$\left|V\right|$的规模在百万级别。一个简单的想法是我们可以进行近似计算。

对于每个训练步骤，与其遍历整个词汇表，倒不如只采样几个负例。我们从噪声分布$P_n(w)$（其概率与词汇表频率的排序相匹配）中“采样”。为了扩充我们引入负采样的问题的公式，需要调整目标函数、梯度、更新策略。

虽然负采样是基于$\text{skip-gram}$模型的，但它实际上是在优化一个不同的目标函数。考虑一个单词-上下文对$(w,c)$。这一对来自训练数据吗？我们用$P(D=1|w, c)$表示$(w,c)$来自语料库数据的概率。相应地，我们用$P(D=0|w, c)$表示$(w,c)$不来自语料库数据的概率。首先，我们使用$\text{sigmoid}$函数对$P(D=1|w, c)$进行建模：
$$
P(D=1|w,c,\theta)=\sigma(v_c^T v_w)=\dfrac{1}{1+e^{(-v_c^T v_w)}}
$$
现在，构建一个新的目标函数，如果一个单词和上下文确实存在于语料库数据中，则尝试最大化它存在的概率。如果它确实不在语料库数据中，则最大化它不存在的概率。我们对这两个概率采用一个简单的最大似然方法。(这里我们取$\theta$作为模型的参数，在本例中是$\mathcal{V}$和$\mathcal{U}$。)
$$
\begin{aligned} \theta & =\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}} P(D=0 \mid w, c, \theta) \\ & =\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}}(1-P(D=1 \mid w, c, \theta)) \\ & =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 \mid w, c, \theta)+\sum_{(w, c) \in \tilde{D}} \log (1-P(D=1 \mid w, c, \theta)) \\ & =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_w^T v_c\right)}\right) \\ & =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_w^T v_c\right)}\right) \end{aligned}
$$
注意，最大化似然与最小化负对数似然相同：
$$
J=-\sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}-\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_w^T v_c\right)}\right)
$$
请注意，$\tilde{D}$是一个“假的”或“否定的”语料库。像“高汤煮鱼是玩具”这样的不自然的句子，出现的概率很低。我们可以通过从词库中随机抽样来生成$\tilde{D}$。

对于$\text{skip-gram}$来说，在给定用来观察上下文词$c−m + j$的中心单词$c$的情况下，新目标函数是：
$$
-\log \sigma\left(u_{c-m+j}^T \cdot v_c\right)-\sum_{k=1}^K \log \sigma\left(-\tilde{u}_k^T \cdot v_c\right)
$$
对于$\text{CBOW}$来说，在给定用来观察中心词$u_c$的上下文向量$\hat{v}=\frac{v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}}{2m}$的情况下的，新的目标函数为：
$$
\begin{aligned}-\log\sigma(u_c^T\cdot\hat{v})-\sum\limits_{k=1}^K\log\sigma(-\tilde{u}_k^T\cdot\hat{v})\end{aligned}
$$
在上述公式中，$\{\tilde{u}_k|\text{}k=1\ldots K\}$取自$P_n(w)$。

### 5. 分层Softmax

在实践中，分层$\text{softmax}$往往对低频词更有效，而负采样对高频词和低维向量更有效。

分层$\text{softmax}$使用二叉树来表示词汇表中的所有单词。树的每个叶子都是一个单词，从根到叶子有一条唯一的路径。在这个模型中，没有单词的输出表示。相反，图的每个节点(除了根节点和叶节点)都与模型将要学习的向量相关联。

在这个模型中，单词$w$的向量为$w_i$的概率$P(w|w_i)$等于随机从根节点开始，结束于$w_i$对应的叶节点的概率。用这种方法计算概率的主要优点是代价仅为$O(\log(|V|))$，对应于路径的长度。

让我们引入一些符号。设$L(w)$为从根节点到叶子节点$w$的路径的节点数（不算根节点）。设这条路径上的第$i$个节点$n(w,i)$的向量为$v_{n(w,i)}$。所以$n(w, 1)$是根节点，而$n(w,L(w))$是$w$的父节点。现在对于每个内部节点$n$，我们任意选择它的一个子节点并称之为$\text{ch}(n)$。然后，我们可以计算概率为：
$$
P\left(w \mid w_i\right)=\prod_{j=1}^{L(w)-1} \sigma\left([n(w, j+1)=\operatorname{ch}(n(w, j))] \cdot v_{n(w, j)}^T v_{w_i}\right)
$$
其中，
$$
[x]=\begin{cases}1\text{ if }x\text{ is true}\\ -1\text{ otherwise}\end{cases}
$$
$\sigma(\cdot)$是$\text{softmax}$函数。

这个公式非常稠密，让我们更仔细地研究一下

首先，我们根据根节点$(n(w, 1))$到叶节点$\text{leaf}(w)$路径的形状计算乘积。如果我们假设$\text{ch}(n)$总是$n$的左节点，那么当路径向左时，术语$[n(w, j + 1) = \text{ch}(n(w, j))]$返回1，如果向右，返回-1。

此外，术语$[n(w, j + 1) = \text{ch}(n(w, j))]$提供了规范化。在节点n处，如果我们将前往左节点和右节点的概率相加，对于任意值的$v_n^Tv_{wi}$，有
$$
\sigma(v_n^T v_{w_i})+\sigma(-v_n^T v_{w_i})=1
$$
归一化还确保$\sum_{w=1}^{|V|}P(w|w_i)=1\text{}$，就像原始的$\text{softmax}$一样。

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230128194415794.png" alt="image-20230128194415794" style="zoom: 33%;" />

最后，我们使用点积比较输入向量$v_{wi}$与每个内部节点向量$v^T_{n(w,j)}$的相似性。让我们来看一个例子。以图中的$w_2$为例，我们必须先取两条左边，再取一条右边，才能从根结点到达$w_2$，因此：
$$
\begin{aligned}P(w_2|w_i)&=p(n(w_2,1),\text{left})\cdot p(n(w_2,2),\text{left})\cdot p(n(w_2,3),\text{right})\\ &=\sigma(v_{n(w_2,1)}^Tv_{w_i})\cdot\sigma(v_{n(w_2,2)}^T v_{w_i})\cdot\sigma(-v_{n(w_2,3)}^T v_{w_i})\end{aligned}
$$
为了训练模型，我们的目标仍然是最小化负对数似然$1-\log P(w|w_i)$。但我们不是更新每个单词的输出向量，而是更新二叉树中从根节点到叶节点路径上的节点的向量。

该方法的速度取决于二叉树的构建方式和叶节点的单词分配方式。

## 单词表示的全局向量（GloVe）

### 1. 比较

到目前为止，我们已经学习两类主要的词嵌入的方法。第一类是基于计数的，依赖于矩阵分解(如$\text{LSA}$、$\text{HAL}$)。这些方法有效地利用了全局统计的信息，主要用于捕获单词相似性，在单词类比等任务上表现不佳。另一类方法基于浅窗口(例如$\text{skip-gram}$和$\text{CBOW}$模型)，它们通过在局部上下文窗口中进行预测来学习词嵌入。这些模型表现出了捕获单词相似度以外的复杂语言模式的能力，但未能利用全局共现统计。

相比之下，$\text{GloVe}$由加权最小二乘模型组成，该模型基于全局词-词共现计数进行训练，从而有效地利用了统计量。该模型产生一个具有有效子结构的词向量空间。该方法在单词类比任务上表现出了最先进的性能，并在多个单词相似度任务上优于现有的其他方法。

### 2. 共现矩阵

让$X$表示词-词**共现矩阵**（Co-occurrence Matrix），其中$X_{ij}$表示词$j$在词$i$的上下文中出现的次数。让$X_i=\sum_kX_{ik}$是任何词$k$在词$i$的上下文中出现的次数。最后，让$P_{ij} = P(w_j|w_i) = \frac{X_{ij}}{X_{i}}$是词$j$在词$i$的上下文中出现的概率。对于大型语料库来说，这一次的计算成本可能很高，但这是一次性的前期成本。

### 3. 最小二乘目标函数

回顾一下，对于$\text{skip-gram}$模型，我们使用$\text{softmax}$来计算单词$j$出现在单词$i$的上下文中的概率，
$$
Q_{i j}=\frac{\exp \left(\vec{u}_{j}^{T} \vec{v}_{i}\right)}{\sum_{w=1}^{W} \exp \left(\vec{u}_{w}^{T} \vec{v}_{i}\right)}
$$
训练以在线、随机的方式进行，但隐含的全局交叉熵损失可以计算为，
$$
J=-\sum_{i \in \text { corpus }} \sum_{j \in \text { context }(i)} \log Q_{i j}
$$
由于相同的词$i$和$j$可以在语料库中多次出现，因此，将$i$和$j$的相同值归为一组是比较有效的方式，
$$
J=-\sum_{i=1}^{W} \sum_{j=1}^{W} X_{i j} \log Q_{i j}
$$
其中共现频率的值由共现矩阵$X$给出。交叉熵损失的一个重要缺点是，它需要对分布$Q$进行适当的归一化，这涉及到对整个词汇的昂贵求和。相反，如果我们使用最小平方目标函数，就不需要再对$P$和$Q$归一化了，
$$
\hat{J}=\sum_{i=1}^{W} \sum_{j=1}^{W} X_{i}\left(\hat{P}_{i j}-\hat{Q}_{i j}\right)^{2}
$$
其中$\hat{P}_{ij}=X_{ij}$和$\hat{Q}_{ij}=\exp{ \left(\vec{u}_{j}^{T} \vec{v}_{i}\right)}$为非归一化分布。这种公式引入了一个新的问题--$X_{ij}$经常取很大的值，使优化变得十分困难。一个有效的改变是最小化$\hat{P}$和$\hat{Q}$的对数的平方误差，
$$
\begin{aligned}
\hat{J} & =\sum_{i=1}^W \sum_{j=1}^W X_i\left(\log (\hat{P})_{i j}-\log \left(\hat{Q}_{i j}\right)\right)^2 \\
& =\sum_{i=1}^W \sum_{j=1}^W X_i\left(\vec{u}_j^T \vec{v}_i-\log X_{i j}\right)^2
\end{aligned}
$$
通过大量实验发现，加权因子$X_i$并不保证是最佳的。相反，可以引入了一个更普遍的加权函数，可以自由地认为它也取决于上下文词，
$$
\begin{aligned}\hat{J}=\sum_{i=1}^{W}\sum_{j=1}^{W}f(X_{ij})(\vec{u}_j^T\vec{v}_i-\log X_{ij})^2\end{aligned}
$$

### 4. 总结

总之，$\text{GloVe}$模型通过只训练单词共现矩阵中的非零元素，有效地利用了全局统计信息，并产生了一个具有意义的子结构的向量空间。在相同的语料库、词汇量、窗口大小和训练时间下，它在单词类比任务上的表现一直优于$\text{word2vec}$。它能更快地取得更好的结果，而且无论速度快慢，都能获得最佳结果。

# 词向量的评估

到目前为止，我们已经讨论了$\text{word2vec}$和$\text{GloVe}$等方法，以训练和发现语义空间中自然语言单词的潜在向量表示。在本节中，我们将讨论如何量化评估这些技术产生的词向量的质量。

## 内在评估和外在评估

词向量的**内在评估**（Intrinsic Evaluation）是对嵌入技术(如$\text{word2vec}$或$\text{GloVe}$)在特定中间子任务(如类比补全)上生成的一组词向量的评估。这些子任务通常简单且计算速度快。内在评估通常应该返回给我们一个数字，表明这些词向量在评估子任务上的性能。

**动机**: 考虑一个例子，我们的最终目标是创建一个使用词向量作为输入的问答系统。一种方法是训练一个机器学习系统:

>1. 将单词作为输入
>2. 将它们转换为词向量。
>3. 使用词向量作为复杂机器学习系统的输入
>4. 将该系统输出的词向量映射回自然语言单词
>5. 产生单词作为答案

在制作这样一个先进的问答系统的过程中，我们需要创建最优的词向量表示，因为它们将用于下游子系统(如深度神经网络)。在实践中，我们需要调整$\text{word2vec}$子系统中的许多超参数(例如词向量表示的维度)。虽然理想的方法是在$\text{word2vec}$子系统中发生任何参数变化后重新训练整个系统，但从工程角度来看，这是不切实际的，因为机器学习系统(在步骤3中)通常是一个具有数百万参数的深度神经网络，需要很长时间来训练。在这种情况下，我们希望提出一种简单的内在评估技术，可以提供单词到词向量子系统的“好坏”的衡量标准。显然，一个要求是内在评价与最终任务表现呈正相关。

词向量的**外在评估**（Extrinsic Evaluation）是对一种嵌入技术在实际任务上生成的一组词向量的评估。这些任务通常都很复杂，计算速度很慢。使用我们上面的例子，允许评估问题答案的系统是外部评估系统。通常，对表现不佳的外部评估系统进行优化，不能让我们确定哪个具体子系统有错误，这激发了对内部评估的需求。

## 内在评估示例：词向量类比

词向量内在评估的一个很受众的选择是在词向量类比任务上的性能。在一个词向量的类比中，我们得到了一个不完全的类比:
$$
a:b::c:?
$$
内在评估系统识别最大化余弦相似度的词向量：
$$
\begin{aligned}d=\underset{i}{\text{argmax}}\frac{(x_b-x_a+x_c)^Tx_i}{\left\|x_b-x_a+x_c\right\|}\end{aligned}
$$
这个指标有一个直观的解释。理想情况下，我们希望$x_b−x_a = x_d−x_c$（例如，queen-king=actress-actor）。这意味着我们需要$x_b−x_a + x_c = x_d$。因此，向量$x_d$最大化了两个词向量之间的归一化点积(即余弦相似度)。

使用内在评估技术，如词向量类比，应谨慎处理(记住用于预训练的语料库的各个方面)。例如，考虑以下形式的类比:

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230202225942451.png" alt="image-20230202225942451" style="zoom:33%;" />

在上面的许多情况下，美国有多个同名的城市/城镇/村庄。因此，许多州都有资格成为正确答案。例如，美国至少有10个地方叫Phoenix，因此，Arizona不一定是唯一正确的答案。现在让我们考虑这种形式的类比:
$$
\text{Capital City 1 : Country 1 : : Capital City 2 : Country 2}
$$
在之前的许多案例中，这项任务产生的结果城市只是最近的首都。例如，1997年之前， Kazakhstan的首都是Almaty。因此，如果语料库有日期，我们可以预测其他问题。

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230202232315293.png" alt="image-20230202232315293" style="zoom:33%;" />

前面两个例子演示了如何使用词向量进行语义测试。我们还可以使用词向量类比来测试语法。下面的内在评估测试了词向量捕捉最高级形容词概念的能力:

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230202232527110.png" alt="image-20230202232527110" style="zoom:33%;" />

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230202233844942.png" alt="image-20230202233844942" style="zoom:33%;" />

## 内在评估调整示例：类比评估

我们现在探索词向量嵌入技术（如$\text{word2vec}$和$\text{GloVe}$）中的一些超参数，这些超参数可以使用内部评估系统（如类比补全系统）进行调整。让我们看看在相同的超参数下，词向量生成的不同方法在类比评估任务中的具体表现(在最新的研究工作中)：

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230202233928026.png" alt="image-20230202233928026" style="zoom:33%;" />

观察上面的表格，我们可以做3个主要的结论：

- **外部系统的性能严重依赖于用于词嵌入的模型**：这是一个可以预见的结果，因为不同的方法使用根本不同的属性(如共现计数、奇异向量等)将单词嵌入到向量中。
- **语料库越大，性能越高**：这是因为嵌入技术通过看到更多的示例获得了更多的经验。例如，一个类比补全示例如果之前没有遇到过测试单词，就会产生不正确的结果。

- **超低维词向量的性能较低**：低维词向量无法捕捉语料库中不同单词的不同含义。这可以被视为模型复杂度过低产生的高偏差问题。例如，让我们考虑单词“king”, “queen”, “man”, “woman”。直观地说，我们需要使用两个维度，如“性别”和“领导能力”，将它们编码为2位的词向量。任何更低的值都无法捕捉到这四个单词之间的语义差异。

下图展示了更大的语料库如何提高准确性：

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230202234728470.png" alt="image-20230202234728470" style="zoom: 67%;" />

下图展示了使用$\text{GloVe}$时其他超参数如何影响精度：

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230202234915896.png" alt="image-20230202234915896" style="zoom:67%;" />

## 内在评估示例：相关性评估

评估词向量质量的另一种简单方法是让人们在固定的尺度(比如0-10)下评估两个单词之间的相似度，然后将其与相应词向量之间的余弦相似度进行比较。这已经在包含人工判断调查数据的各种数据集上完成。

<img src="https://sjtu-mlj.oss-cn-shanghai.aliyuncs.com/typora/windows/image-20230202235357793.png" alt="image-20230202235357793" style="zoom:33%;" />

## 进一步阅读：处理歧义

有人可能想知道，我们如何处理用不同向量捕获相同单词以供其在自然语言中的不同用途的情况。例如，“run”既是名词又是动词，根据上下文的不同，其用法和解释也有所不同。《Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al, 2012)》描述了如何在自然语言处理中处理该情况。这个方法的本质如下:

> 1. 收集一个单词出现的所有固定大小的上下文窗口(例如，出现前5次，出现后5次)
> 2. 每个上下文由上下文词向量的加权平均值表示(使用idf加权)。
> 3. 应用球形k均值对这些上下文表示进行聚类。
> 4. 最后，每个单词被重新标记到与其关联的簇，并用于训练该簇的单词表示。

关于这个主题的更严格的处理，可以参考原始论文。

# 为外部任务进行训练
